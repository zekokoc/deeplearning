# -*- coding: utf-8 -*-
"""deepLearning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Isw7rxdywM95NKUYeBKIyJf2CmhntoP0

# **İMPORT**
"""

from google.colab import drive
drive.mount('/gdrive')
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import glob
import cv2
import os

from sklearn.model_selection import train_test_split, GridSearchCV , cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report , mean_absolute_error , mean_squared_error

from sklearn import model_selection
from warnings import filterwarnings
filterwarnings('ignore')

from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv2D, MaxPooling2D, Dropout
from keras.models import Sequential

import tensorflow as tf
from tensorflow import keras

"""# **DATA TRANSACTIONS**"""

print(os.listdir("/gdrive/MyDrive/lung_image_sets"))

DATADIR = "/gdrive/MyDrive/lung_image_sets"   # veri yolu
CATEGORIES = ['lung_benign tissue', 'lung_squamous cell carcinoma', 'lung_adenocarcinoma'] # görüntü kategorileri

IMG_SIZE = 64 # fonksiyondan önce tanımlanan görüntü boyutu

all_data =[]
# veri kümlerinin çekilmesi ve dönüştürülmesi
# img size ayracına göre

# all_data isimli boş listeye veri yoluna göre kategorik olarak sınıf numaralarıyla ekleme işlemi yapılacak

def create_all_data():
    for category in CATEGORIES:
        path=os.path.join(DATADIR, category)
        class_num=CATEGORIES.index(category)
        for img in os.listdir(path):
            try:
                img_array=cv2.imread(os.path.join(path,img))
                new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))
                all_data.append([new_array,class_num])
            except Exception as e:
                pass
create_all_data()

print("Toplam Görüntü Sayısı: ", len(all_data)) # toplam görüntü sayısı

X = [] # görüntünün kendisi (eğitilecek olan veriler)
y = [] # sınıf etiketleri    (karşılık gelen sınıf isimleri)


# döngüde all_data içerisinden ayrım yapılmaktadır
for categories, label in all_data:
  X.append(categories)
  y.append(label)

#buraya xload y load olanı dene ann için

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state =42) # x_test %20 , x_train %80
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.10, random_state=42) # validation olarak ayrılan veriler(eğitim veri setinin %10'u)
# eğitilecek verileriler üzerinden validation ayrımı yapılacağı için parametreler x_train ve y_train olarak verilmiştir

# numpy array'e çevirme işlemi
# görüntü işlemede görüntü sınıflandırma yapabilmek için çevirme işlemi yapılmıştır

x_train = np.array(x_train)
x_test = np.array(x_test)
y_train = np.array(y_train)
y_test = np.array(y_test)
y_test = y_test.reshape(-1,1)
x_val = np.array(x_val)
y_val = np.array(y_val)
y_val = y_val.reshape(-1,1)
y_train = y_train.reshape(-1,1)
y_test = y_test.reshape(-1,1)

# dönüştürülen numpy arraylerin boyut kontrolü (.shape)


print(x_train.shape)
print(x_test.shape)
print(x_val.shape)

print(y_train.shape)
print(y_test.shape)
print(y_val.shape)

number_of_train = x_train.shape[0]
number_of_test = x_test.shape[0]
#x_test_flatten = x_test.reshape(number_of_test,x_test.shape[1]*x_test.shape[2])
#x_train_flatten = x_train.reshape(number_of_train,x_train.shape[0])

"""# **CNN**"""

# normalize

x_train = x_train / 255.0 # bir resmin alacağı değer max 255 olduğu için 255 e böldük
x_test = x_test / 255.0
print(x_train.shape)
print(x_test.shape)

# kategorik değere çevirme işlemi ,(to_categorical)
# label encoding
train_yCl = tf.keras.utils.to_categorical(y_train, num_classes=9)
test_yCl = tf.keras.utils.to_categorical(y_test, num_classes=9)
valid_yCl = tf.keras.utils.to_categorical(y_val, num_classes=9)

"""HİPER PARAMETRE OPTİMZASYONU

from hyperopt import fmin, tpe, hp
from keras.preprocessing.image import ImageDataGenerator

# Veri augmentasyonunu uygulayın
datagen = ImageDataGenerator(
    rotation_range=0.5,
    zoom_range=0.5,
    width_shift_range=0.5,
    height_shift_range=0.5,
    horizontal_flip=False,
    vertical_flip=False
)
datagen.fit(x_train)

# Optimizasyon işlevini tanımlayın
def optimize(params):
    model = Sequential()

    model.add(Conv2D(8, (3, 3), padding='same', input_shape=(IMG_SIZE, IMG_SIZE, 3)))
    model.add(Activation('relu'))
    model.add(Conv2D(8, (3, 3), padding='same'))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(BatchNormalization())
    model.add(Dropout(0.1))

    model.add(Conv2D(8, (3, 3), padding='same'))
    model.add(Activation('relu'))
    model.add(Conv2D(8, (3, 3), padding='same'))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(BatchNormalization())
    model.add(Dropout(0.1))

    model.add(Conv2D(16, (3, 3), padding='same'))
    model.add(Activation('relu'))
    model.add(Conv2D(16, (3, 3), padding='same'))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(BatchNormalization())
    model.add(Dropout(0.1))

    model.add(Flatten())
    model.add(Dense(1024))
    model.add(Activation('relu'))
    model.add(Dropout(0.4))
    model.add(Dense(1024))
    model.add(Activation('relu'))
    model.add(Dense(9))
    model.add(Activation('softmax'))

    optimizer = Adam(learning_rate=params['learning_rate'])
    model.compile(loss="categorical_crossentropy", optimizer=optimizer, metrics=['accuracy'])

    history = model.fit(datagen.flow(x_train, train_yCl, batch_size=64),
                        epochs=10,
                        validation_data=(x_val, valid_yCl),
                        verbose=0)

    # Hyperopt, minimize işlemi gerçekleştirir, bu nedenle negatif doğruluk kullanıyoruz
    return -history.history['val_accuracy'][-1]

# Parametre aralıklarını tanımlayın
space = {
    'learning_rate': hp.loguniform('learning_rate', -6, -3)
}

# Optimizasyonu gerçekleştirin
best = fmin(fn=optimize, space=space, algo=tpe.suggest, max_evals=50)

# En iyi hiperparametreleri yazdırın
print("En iyi hiperparametreler:", best)
"""

# tensorflow ile cnn tasarım işlemi
from sklearn.metrics import confusion_matrix
import itertools

from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv2D, MaxPooling2D, Dropout
from keras.optimizers import RMSprop , Adam
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau

# from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv2D, MaxPooling2D, Dropout
# from keras.models import Sequential

model = Sequential()

model.add(Conv2D(8, (3,3), padding='same', input_shape=(IMG_SIZE, IMG_SIZE, 3)))
model.add(Activation('relu'))
model.add(Conv2D(8, (3,3), padding='same'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(BatchNormalization())
model.add(Dropout(0.1))

model.add(Conv2D(8, (3,3), padding='same'))
model.add(Activation('relu'))
model.add(Conv2D(8, (3,3), padding='same'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(BatchNormalization())
model.add(Dropout(0.1))

model.add(Conv2D(16, (3,3), padding='same'))
model.add(Activation('relu'))
model.add(Conv2D(16, (3,3), padding='same'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(BatchNormalization())
model.add(Dropout(0.1))

model.add(Flatten())
model.add(Dense(1024))
model.add(Activation('relu'))
model.add(Dropout(0.4))
model.add(Dense(1024))
model.add(Activation('relu'))
model.add(Dense(9))
model.add(Activation('softmax'))



"""
model = Sequential()

model.add(Conv2D(filters = 8 , kernel_size = (5,5),padding = 'Same', activation='relu', input_shape = (IMG_SIZE,IMG_SIZE,3)))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(BatchNormalization())
model.add(Dropout(0.25))

model.add(Conv2D(filters = 16, kernel_size = (3,3), padding='same'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides = (2,2))) #strides basamak sayısı
model.add(BatchNormalization())
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(256))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(10))
model.add(Activation('softmax'))
"""
optimizer = Adam(learning_rate=0.001, beta_1=0.9 , beta_2 = 0.999)

model.compile(loss="categorical_crossentropy", optimizer= optimizer, metrics=['accuracy'])

# data agumentation

datagen = ImageDataGenerator (
    featurewise_center = False ,
    samplewise_center = False ,
    featurewise_std_normalization= False ,
    samplewise_std_normalization=False,
    zca_whitening=False ,
    rotation_range= 0.5 ,
    zoom_range=0.5,
    width_shift_range=0.5,
    height_shift_range=0.5,
    horizontal_flip=False,
    vertical_flip = False)

datagen.fit(x_train)

callback_list = [
    keras.callbacks.ModelCheckpoint(
        filepath='model.h5',
        monitor = 'val_accuracy', save_best_only=True, verbose=3
    ),
    keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=3)
]

history = model.fit(x_train , train_yCl ,
                    batch_size=64 ,
                    epochs = 10 ,
                    validation_data = (x_val , valid_yCl),
                    callbacks = callback_list
)

# test train ve validation doğruluk oranları

score_valid = model.evaluate(x_val, valid_yCl)
print("Validation Accuracy: ", score_valid[1])

score_test = model.evaluate(x_test, test_yCl)
print("(Test)Validation Accuracy: ", score_test[1])

score_train = model.evaluate(x_train, train_yCl)
print("(Train)Validation Accuracy: ", score_train[1])

# visualization loss
plt.plot(history.history['val_loss'], color='b', label="validation loss")
plt.title("Test Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

# visualization accuracy
"""
plt.plot(history.history['val_accuracy'], color='r', label="validation accuracy")
plt.title("Test Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()
"""
df_history = pd.DataFrame(history.history)
sns.lineplot(data=df_history[['accuracy','val_accuracy']], palette="tab10", linewidth=2.5);

import itertools

def plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Reds):

    plt.figure(figsize=(10,10))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        cm = np.around(cm, decimals=2)
        cm[np.isnan(cm)] = 0.0
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

def evaluate_model(model, x_test, y_test):
    # Accuracy Score
    y_pred = model.predict(x_test)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_test_classes = np.argmax(y_test, axis=1)
    accuracy = accuracy_score(y_test_classes, y_pred_classes)
    print(f"Accuracy: {accuracy}")

    # Precision Score
    precision = precision_score(y_test_classes, y_pred_classes, average='weighted')
    print(f"Precision: {precision}")

    # Recall Score
    recall = recall_score(y_test_classes, y_pred_classes, average='weighted')
    print(f"Recall: {recall}")

    # F1 Score
    f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')
    print(f"F1-Score: {f1}")

    # Classification Report
    print("Classification Report:")
    print(classification_report(y_test_classes, y_pred_classes))

    # Confusion Matrix
    cm = confusion_matrix(y_test_classes, y_pred_classes)
    classes = ['lung_benign tissue', 'lung_squamous cell carcinoma', 'lung_adenocarcinoma']
    plot_confusion_matrix(cm, classes, title='Confusion Matrix')

# Modeli değerlendir
evaluate_model(model, x_test, y_test)

"""# **ANN (Artificial Neural Network)**

x_train = np.random.rand(10800, 64, 64, 3)
x_test = np.random.rand(3000, 64, 64, 3)
x_val = np.random.rand(1200, 64, 64, 3)
# Matrisi düzleştirme
x_train = x_train.reshape(10800, -1)
x_test = x_test.reshape(3000,-1)
x_val = x_val.reshape(1200,-1)
# Düzleştirilmiş matrisin boyutları
print(x_train.shape)
print(x_test.shape)
print(x_val.shape)

# ANN (Artificial Neural Network)

# initalize parameters and layer size (weight için random bir şekilde initalize yapılacak . (0  tanımlanmayacak çeşitlilik yaratmak adına))
# initalize yüksek tanımlanmayacak tanh fonk. güncelleme süresi kısaltmak adına.

def initialize_parameters_and_layer_sizes_NN(x_train, y_train):
    parameters = {"weight1": np.random.randn(3,x_train.shape[0]) * 0.1,
                  "bias1": np.zeros((3,1)),
                  "weight2": np.random.randn(y_train.shape[0],3) * 0.1,
                  "bias2": np.zeros((y_train.shape[0],1))}
    return parameters
"
"""

# sigmoid fonksiyon
"""
def sigmoid(z):
  y_head = 1/(1+np.exp(-z))
  return y_head  # y_head = sigmoid(z)
"""

# forward propagation
"""
def forward_propagation_NN(x_train, parameters):

    Z1 = np.dot(parameters["weight1"],x_train) + parameters["bias1"]
    A1 = np.tanh(Z1)
    Z2 = np.dot(parameters["weight2"],A1) + parameters["bias2"]
    A2 = sigmoid(Z2)

    cache = {"Z1": Z1,
             "A1": A1,
             "Z2": Z2,
             "A2": A2}

    return A2, cache
"""

# compute cost function
"""
def compute_cost_NN(A2, Y, parameters):
    logprobs = np.multiply(np.log(A2),Y)
    cost = -np.sum(logprobs)/Y.shape[1]
    return cost
"""

# backward propagation
"""
def backward_propagation_NN(parameters, cache, X, Y):

    dZ2 = cache["A2"]-Y
    dW2 = np.dot(dZ2,cache["A1"].T)/X.shape[1]
    db2 = np.sum(dZ2,axis =1,keepdims=True)/X.shape[1]
    dZ1 = np.dot(parameters["weight2"].T,dZ2)*(1 - np.power(cache["A1"], 2))
    dW1 = np.dot(dZ1,X.T)/X.shape[1]
    db1 = np.sum(dZ1,axis =1,keepdims=True)/X.shape[1]
    grads = {"dweight1": dW1,
             "dbias1": db1,
             "dweight2": dW2,
             "dbias2": db2}
    return grads
"""

# update parameters
"""
def update_parameters_NN(parameters, grads, learning_rate = 0.01):
    parameters = {"weight1": parameters["weight1"]-learning_rate*grads["dweight1"],
                  "bias1": parameters["bias1"]-learning_rate*grads["dbias1"],
                  "weight2": parameters["weight2"]-learning_rate*grads["dweight2"],
                  "bias2": parameters["bias2"]-learning_rate*grads["dbias2"]}

    return parameters
"""

# prediction
"""
def predict_NN(parameters,x_test):
    # x_test is a input for forward propagation
    A2, cache = forward_propagation_NN(x_test,parameters)
    Y_prediction = np.zeros((1,x_test.shape[0]))
    # if z is bigger than 0.5, our prediction is sign one (y_head=1),
    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),
    for i in range(A2.shape[0]):
        if A2[0,i]<= 0.5:
            Y_prediction[0,i] = 0
        else:
            Y_prediction[0,i] = 1

    return Y_prediction
"""

# create ann model (two layer use)
"""
def two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations):
    cost_list = []
    index_list = []
    #initialize parameters and layer sizes
    parameters = initialize_parameters_and_layer_sizes_NN(x_train, y_train)

    for i in range(0, num_iterations):
         # forward propagation
        A2, cache = forward_propagation_NN(x_train,parameters)
        # compute cost
        cost = compute_cost_NN(A2, y_train, parameters)
         # backward propagation
        grads = backward_propagation_NN(parameters, cache, x_train, y_train)
         # update parameters
        parameters = update_parameters_NN(parameters, grads)

        if i % 100 == 0:
            cost_list.append(cost)
            index_list.append(i)
            print ("Cost after iteration %i: %f" %(i, cost))
    plt.plot(index_list,cost_list)
    plt.xticks(index_list,rotation='vertical')
    plt.xlabel("Number of Iterarion")
    plt.ylabel("Cost")
    plt.show()

    # predict
    y_prediction_test = predict_NN(parameters,x_test)
    y_prediction_train = predict_NN(parameters,x_train)

    # Print train/test Errors
    print("train accuracy: {} %".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))
    print("test accuracy: {} %".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))
    return parameters

parameters = two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations=3)
"""

from keras.models import Sequential
from keras.layers import Dense, Flatten, Dropout
from keras.optimizers import Adam
from keras.preprocessing.image import ImageDataGenerator

# Model oluşturma
model = Sequential()

# Giriş Katmanı
model.add(Flatten(input_shape=(IMG_SIZE, IMG_SIZE, 3)))

# İlk Gizli Katman
model.add(Dense(1024, activation='sigmoid'))
model.add(Dropout(0.5))

# İkinci Gizli Katman
model.add(Dense(512, activation='sigmoid'))
model.add(Dropout(0.5))

# Üçüncü Gizli Katman
model.add(Dense(256, activation='sigmoid'))
model.add(Dropout(0.5))

# Dördüncü Gizli Katman
model.add(Dense(128, activation='sigmoid'))
model.add(Dropout(0.5))

# Beşinci Gizli Katman
model.add(Dense(64, activation='sigmoid'))
model.add(Dropout(0.5))

# Altıncı Gizli Katman
model.add(Dense(32, activation='sigmoid'))
model.add(Dropout(0.5))

# Yedinci Gizli Katman
model.add(Dense(16, activation='sigmoid'))
model.add(Dropout(0.5))

# Çıkış Katmanı
model.add(Dense(3, activation='tanh'))

# Modeli Derleme
optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Model Yapısını Görüntüleme
model.summary()

# Modeli Eğitme
history = model.fit(x_train, y_train, epochs=5, validation_data=(x_val, y_val))

# Eğitim ve Doğrulama Kayıplarını Görselleştirme
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Eğitim ve Doğrulama Başarımlarını Görselleştirme
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Test Veri Seti ile Modeli Değerlendirme
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f'Test Loss: {test_loss}')
print(f'Test Accuracy: {test_accuracy}')

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

# Modeli Test Verisi ile Değerlendirme
y_pred = model.predict(x_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# Accuracy
test_accuracy = accuracy_score(y_test, y_pred_classes)
print(f'Test Accuracy: {test_accuracy}')

# Precision
precision = precision_score(y_test, y_pred_classes, average='weighted')
print(f'Precision: {precision}')

# Recall
recall = recall_score(y_test, y_pred_classes, average='weighted')
print(f'Recall: {recall}')

# F1 Score
f1 = f1_score(y_test, y_pred_classes, average='weighted')
print(f'F1 Score: {f1}')

# Karmaşıklık Matrisi
conf_matrix = confusion_matrix(y_test, y_pred_classes)
print(f'Confusion Matrix:\n{conf_matrix}')

# Classification Report
class_report = classification_report(y_test, y_pred_classes)
print(f'Classification Report:\n{class_report}')

"""# **TRANSFER LEARNİNG**"""

from keras.applications import VGG16
from keras.models import Model
from keras.layers import Dense, GlobalAveragePooling2D
from keras.optimizers import Adam

# VGG16 modelini kullanma
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))

# Modelin üzerine ekleyeceğimiz katmanlar
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
predictions = Dense(3, activation='softmax')(x)

# Transfer learning modelini oluşturma
model = Model(inputs=base_model.input, outputs=predictions)

# Önceden eğitilmiş katmanları dondurma
for layer in base_model.layers:
    layer.trainable = False

# Modeli derleme
model.compile(optimizer=Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Modeli eğitme
history_transfer_learning = model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))

# Eğitim ve Doğrulama Kayıplarını Görselleştirme
plt.plot(history_transfer_learning.history['loss'], label='Training Loss')
plt.plot(history_transfer_learning.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Eğitim ve Doğrulama Başarımlarını Görselleştirme
plt.plot(history_transfer_learning.history['accuracy'], label='Training Accuracy')
plt.plot(history_transfer_learning.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Test verisi ile modeli değerlendirme
test_loss_transfer_learning, test_accuracy_transfer_learning = model.evaluate(x_test, y_test)
print(f'Test Loss (Transfer Learning): {test_loss_transfer_learning}')
print(f'Test Accuracy (Transfer Learning): {test_accuracy_transfer_learning}')

# Precision, Recall, F1 Score, Confusion Matrix, Classification Report
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report

y_pred_transfer_learning = model.predict(x_test)
y_pred_transfer_learning_classes = np.argmax(y_pred_transfer_learning, axis=1)

precision = precision_score(y_test, y_pred_transfer_learning_classes, average='weighted')
recall = recall_score(y_test, y_pred_transfer_learning_classes, average='weighted')
f1 = f1_score(y_test, y_pred_transfer_learning_classes, average='weighted')
conf_matrix = confusion_matrix(y_test, y_pred_transfer_learning_classes)
class_report = classification_report(y_test, y_pred_transfer_learning_classes)

print(f'Precision (Transfer Learning): {precision}')
print(f'Recall (Transfer Learning): {recall}')
print(f'F1 Score (Transfer Learning): {f1}')
print(f'Confusion Matrix (Transfer Learning):\n{conf_matrix}')
print(f'Classification Report (Transfer Learning):\n{class_report}')